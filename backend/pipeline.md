# AnimationGPT Pipeline

- Run the backend:

```sh
uvicorn main:app --reload --port 8000
```

## Send Request

- First, after the server is up and running, the application will receive a request from the frontend.

- Below is a curl request to test the endpoint;

```sh
curl -X POST "http://127.0.0.1:8000/generate" \
     -H "Content-Type: application/json" \
     -d '{"topic": "Black Holes"}'
```

## AI Pipeline

This AI pipeline will receive the prompt from the user in the frontend, and does the following; 1. Topic detection, 2. Read between the lines to get the context and intention, 3. Generate the prompt request to the animation engine., 4. Generate the questions on the same topic, based on the prompt to the animation engine. Additionally,the pipeline will add generate the notes about the same animation that is to be generated by the animation engine.
Moreover, the pipeline should read the context to also get a grasp of whether the animation will require an interactive funcionality(like when if the animation is about projectile motion, then I the interactivity should be on changing the parameters like launch angle, the initial force[If it is too far fetched, I can impliment it later, no rush! :)])

| Stage                                 | Task                                     | Output                                                |
| ------------------------------------- | ---------------------------------------- | ----------------------------------------------------- |
| 1. Topic Detection                    | Extract core Science subject                | `"Kinematics"`, `"Optics"`, `"Gravity"`               |
| 2. Intention Extraction               | Read between the lines, detect purpose   | `"Demonstrate projectile motion"`                     |
| 3. Scene Blueprint Generation         | Convert idea → objects/actions/timelines | JSON scene design for animation engine                |
| 4. Narration + Learning Questions     | Educational layer for understanding      | Explanations + 2–4 reflection questions               |
| 5. Instruction Notes                  | Additional technical notes               | `"may include interactivity", parameters recommended` |
| 6. Interactivity Detection (Optional) | Detect if user intent implies tinkering  | Angles, mass, friction, velocity…                     |

### Logic Flow

```txt
User natural language prompt
     ⬇
AI Pipeline
┌─────────────────────────────────────────────┐
│ 1. Detect Topic                             │
│ 2. Extract Context & Intention              │
│ 3. Build Animation Prompt                   │
│ 4. Generate Narration + Questions           │
│ 5. Add Notes + Interactivity Check          │
└─────────────────────────────────────────────┘
     ⬇
Animation Engine receives JSON
```

### Topic Detection

The request is run into a system that detects what is is being said in the request. Try to relate the request unique words to the topic in STEM.

To test the topic detection, I added an endpoint, and below is a way to test it:

```sh
curl -X POST "http://127.0.0.1:8000/detect_topic" \
     -H "Content-Type: application/json" \
     -d '{"text": "How does gravity affect falling objects and motion?"}'
```

## Generation of the query to the Animation system

The NLP system generates a request to the animation creator.


The animation is generated with modifications based on what is being learned. The user should be able to adjust parameters to experience learning.

The response is returned back to the frontend and displayed to the user.

Moreover, before the animation is returned to the user in the frontend, the application calls on a system, to generate the script of the learning, in text{pdf, or show a snippet of the top.}

Additionally, the app adds a section wiht some questions based on the animationa generated to foster understanding of the topic in disussion.

If the user takes a step further and needs clarification about the questions presented, then the animation generation is disabled (the explanation is done without animation generation).